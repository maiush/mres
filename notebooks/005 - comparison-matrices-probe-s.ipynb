{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train on the first 50% of article ids (as determined by the get_id_dict function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"../utils\")\n",
    "from constants import *\n",
    "from helper import get_id_dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pickle\n",
    "from typing import List\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "\n",
    "n_subset_dict = {\n",
    "    \"newsroom\": 60,\n",
    "    \"summeval\": 100\n",
    "}\n",
    "n_example_dict = {\n",
    "    \"newsroom\": 7,\n",
    "    \"summeval\": 16\n",
    "}\n",
    "\n",
    "def get_label(scores: List[int]) -> int:\n",
    "    s1, s2 = scores\n",
    "    if s1 > s2: return 1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2: 100%|██████████| 4/4 [00:26<00:00,  6.64s/it]\n",
      "llama3: 100%|██████████| 4/4 [00:25<00:00,  6.47s/it]\n",
      "mistral: 100%|██████████| 4/4 [00:27<00:00,  6.93s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = \"newsroom\"\n",
    "id_dict = get_id_dict(dataset)\n",
    "\n",
    "article_ids = [k for k in id_dict.keys()]\n",
    "train_ids = article_ids[:len(article_ids)//2]\n",
    "test_ids = article_ids[len(article_ids)//2:]\n",
    "prompts = pd.read_json(f\"{gdrive_path}/model_harvesting/prompts_short/{dataset}-mine-compare-1.jsonl\", orient=\"records\", lines=True)\n",
    "train_ixs = prompts.loc[prompts[\"article_id\"].isin(train_ids)].index\n",
    "test_ixs = prompts.loc[prompts[\"article_id\"].isin(test_ids)].index\n",
    "\n",
    "for model_name in [\"llama2\", \"llama3\", \"mistral\"]:\n",
    "    for aspect in tqdm([\"consistency\", \"coherence\", \"fluency\", \"relevance\"], desc=model_name):\n",
    "        prompts[\"label\"] = prompts[aspect].apply(get_label)\n",
    "        c1 = t.load(f\"{gdrive_path}/model_harvesting/{model_name}/activations_short/{dataset}_{aspect}_1.pt\")\n",
    "        c2 = t.load(f\"{gdrive_path}/model_harvesting/{model_name}/activations_short/{dataset}_{aspect}_2.pt\")\n",
    "        data = c1 - c2\n",
    "        y_train, y_test = prompts.loc[train_ixs, \"label\"].values, prompts.loc[test_ixs, \"label\"].values\n",
    "        X_train, X_test = data[train_ixs], data[test_ixs]\n",
    "        perm = t.randperm(len(X_train))\n",
    "        X_train, y_train = X_train[perm], y_train[perm]\n",
    "        lr = LogisticRegression(max_iter=1000)\n",
    "        lr.fit(X_train, y_train)\n",
    "\n",
    "        comparisons = np.zeros((n_subset_dict[dataset]//2, n_example_dict[dataset], n_example_dict[dataset])) + 0.5\n",
    "        for subset_ix, subset_id in enumerate(test_ids):\n",
    "            _condition = prompts[\"article_id\"] == subset_id\n",
    "            example_ids = id_dict[subset_id]\n",
    "            for example1 in example_ids:\n",
    "                example1_ix = example_ids.index(example1)\n",
    "                for example2 in example_ids:\n",
    "                    example2_ix = example_ids.index(example2)\n",
    "                    if example1 == example2: continue\n",
    "                    condition = _condition & (prompts[\"system_id\"].apply(lambda x: x==[example1, example2]))\n",
    "                    ix = prompts.loc[condition].index\n",
    "                    if len(ix) == 0:\n",
    "                        condition = _condition & (prompts[\"system_id\"].apply(lambda x: x==[example2, example1]))\n",
    "                        ix = prompts.loc[condition].index\n",
    "                    p_s1, p_s2 = lr.predict_proba(data[[ix[0]], :]).flatten()\n",
    "                    comparisons[subset_ix, example1_ix, example2_ix] = p_s1\n",
    "                    comparisons[subset_ix, example2_ix, example1_ix] = p_s2\n",
    "        outfile = open(f\"../comparison_matrices/{model_name}/activations-short-{dataset}-{aspect}.pickle\", \"wb\")\n",
    "        pickle.dump(comparisons, outfile); outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pairs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
